from typing import Any, Dict, List, Optional, Tuple
#!/usr/bin/env python3
"""
Smart Patch Processor - Correcteur complet et automatique
Corrige toutes les erreurs identifi√©es et optimise l'architecture
Version: 3.0 - Correcteur universel
"""

import os
import sys
import shutil
import re
from pathlib import Path
from datetime import datetime
import logging

class ComprehensiveFixer:
    """Correcteur complet pour Smart Patch Processor"""
    
    def __init__(self):
        self.script_dir = Path(__file__).parent
        self.backup_dir = self.script_dir / f"comprehensive_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.fixes_applied = []
        self.files_deleted = []
        self.files_created = []
        self.errors_found = 0
        
        # Configuration du logging
        logging.basicConfig(level=logging.INFO, format='%(message)s')
        self.logger = logging.getLogger(__name__)
        
    def run_comprehensive_fix(self) -> bool:
        """Lance la correction compl√®te"""
        print("üîß Smart Patch Processor - Correcteur Complet v3.0")
        print("=" * 70)
        
        try:
            # Phase 1: Pr√©paration
            self._prepare_backup()
            
            # Phase 2: Corrections critiques
            self._fix_critical_errors()
            
            # Phase 3: Consolidation des fichiers
            self._consolidate_files()
            
            # Phase 4: Suppression des fichiers redondants
            self._remove_redundant_files()
            
            # Phase 5: Optimisations finales
            self._apply_final_optimizations()
            
            # Phase 6: Validation
            self._validate_fixes()
            
            self._show_summary()
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur critique: {e}")
            self._restore_backup()
            return False
    
    def _prepare_backup(self):
        """Pr√©pare les sauvegardes"""
        print("\nüìÅ Phase 1: Pr√©paration des sauvegardes")
        
        self.backup_dir.mkdir(exist_ok=True)
        
        # Sauvegarder tous les fichiers Python
        python_files = list(self.script_dir.glob("*.py"))
        for file in python_files:
            if file.name != __file__.split('/')[-1]:  # √âviter de se sauvegarder
                shutil.copy2(file, self.backup_dir)
        
        print(f"   ‚úÖ {len(python_files)} fichiers sauvegard√©s dans {self.backup_dir}")
    
    def _fix_critical_errors(self):
        """Corrige toutes les erreurs critiques identifi√©es"""
        print("\nüö® Phase 2: Correction des erreurs critiques")
        
        # 1. Corriger validation.py
        self._fix_validation_errors()
        
        # 2. Corriger patch_processor_config.py
        self._fix_config_yaml_errors()
        
        # 3. Corriger wizard_mode.py
        self._fix_wizard_imports()
        
        # 4. Corriger line_number_corrector.py
        self._fix_corrector_duplicates()
        
        # 5. Corriger patch_applicator.py
        self._fix_applicator_logic()
        
        print(f"   ‚úÖ {len(self.fixes_applied)} corrections critiques appliqu√©es")
    
    def _fix_validation_errors(self):
        """Corrige les erreurs dans validation.py"""
        file_path = self.script_dir / "validation.py"
        if not file_path.exists():
            return
            
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Corriger la variable non d√©finie original_filename
        old_pattern = r'def sanitize_filename_secure\(filename: str, max_length: int = 100\) -> str:\s*"""Nettoyage s√©curis√© de nom de fichier"""\s*import re\s*import unicodedata'
        
        new_code = '''def sanitize_filename_secure(filename: str, max_length: int = 100) -> str:
    """Nettoyage s√©curis√© de nom de fichier"""
    import re
    import unicodedata
    
    # CORRECTION: Sauvegarder le nom original
    original_filename = filename'''
        
        content = re.sub(old_pattern, new_code, content, flags=re.DOTALL)
        
        # Corriger les imports manquants
        if "from typing import List" not in content:
            content = content.replace(
                "from typing import Union, Optional",
                "from typing import Union, Optional, List"
            )
        
        # Corriger la fonction validate_file_path_secure pour √©viter les doublons
        if content.count("def validate_file_path_secure") > 1:
            # Garder seulement la premi√®re d√©finition compl√®te
            parts = content.split("def validate_file_path_secure")
            if len(parts) > 2:
                # Reconstruire avec seulement la premi√®re fonction compl√®te
                content = parts[0] + "def validate_file_path_secure" + parts[1]
                # Trouver la fin de la premi√®re fonction et garder le reste
                lines = content.split('\n')
                in_function = False
                indent_level = 0
                result_lines = []
                
                for line in lines:
                    if "def validate_file_path_secure" in line:
                        in_function = True
                        indent_level = len(line) - len(line.lstrip())
                        result_lines.append(line)
                    elif in_function and line.strip() and len(line) - len(line.lstrip()) <= indent_level and not line.startswith(' '):
                        in_function = False
                        result_lines.append(line)
                    else:
                        result_lines.append(line)
                
                content = '\n'.join(result_lines)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        self.fixes_applied.append("validation.py: Variable original_filename d√©finie")
        self.fixes_applied.append("validation.py: Imports corrig√©s")
    
    def _fix_config_yaml_errors(self):
        """Corrige les erreurs YAML dans patch_processor_config.py"""
        file_path = self.script_dir / "patch_processor_config.py"
        if not file_path.exists():
            return
            
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Corriger la r√©cursion infinie dans _load_yaml_secure
        problematic_pattern = r'return self\._load_yaml_secure\(content\)'
        if problematic_pattern in content:
            content = re.sub(
                problematic_pattern,
                'return yaml.safe_load(content) if yaml else None',
                content
            )
            self.fixes_applied.append("patch_processor_config.py: R√©cursion YAML corrig√©e")
        
        # Corriger la logique conditionnelle incorrecte
        bad_logic = r'yaml\.dump\(" if "if yaml:" not in content else "yaml\.dump\("'
        if re.search(bad_logic, content):
            content = re.sub(
                bad_logic,
                'yaml.dump(" if yaml else "# yaml.dump("',
                content
            )
            self.fixes_applied.append("patch_processor_config.py: Logique conditionnelle corrig√©e")
        
        # Simplifier _load_config_file_secure
        secure_method_pattern = r'def _load_config_file_secure\(self, config_path: Path\) -> Optional\[Dict\]:.*?(?=def|\Z)'
        
        new_secure_method = '''def _load_config_file_secure(self, config_path: Path) -> Optional[Dict]:
        """Charge un fichier de configuration (YAML ou JSON) de mani√®re s√©curis√©e"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if not content:
                    return None
                
                # V√©rifier la taille pour √©viter DoS
                if len(content.encode('utf-8')) > 10 * 1024 * 1024:  # 10MB max
                    raise ValueError("Config file too large")
                
                # D√©tecter le format et parser s√©curis√©
                if config_path.suffix.lower() in ['.yaml', '.yml']:
                    if yaml is None:
                        print(f"‚ö†Ô∏è PyYAML non disponible, tentative JSON pour {config_path}")
                        return json.loads(content)
                    
                    # Utiliser safe_load directement (pas de m√©thode s√©par√©e)
                    config = yaml.safe_load(content)
                    
                    # Validation de base
                    if not isinstance(config, dict):
                        raise ValueError("Config must be a dictionary")
                    
                    return config
                    
                elif config_path.suffix.lower() == '.json':
                    return json.loads(content)
                else:
                    # Essayer YAML d'abord, puis JSON
                    try:
                        if yaml:
                            return yaml.safe_load(content)
                        else:
                            return json.loads(content)
                    except Exception:
                        return json.loads(content)
                        
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur chargement {config_path}: {e}")
            return None

    '''
        
        content = re.sub(secure_method_pattern, new_secure_method, content, flags=re.DOTALL)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        self.fixes_applied.append("patch_processor_config.py: M√©thode de chargement s√©curis√©e simplifi√©e")
    
    def _fix_wizard_imports(self):
        """Corrige les imports circulaires dans wizard_mode.py"""
        file_path = self.script_dir / "wizard_mode.py"
        if not file_path.exists():
            return
            
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Supprimer l'import probl√©matique get_processor
        content = content.replace(
            "from core import registry, get_processor",
            "from core import registry"
        )
        
        # Corriger les appels √† get_processor()
        content = content.replace(
            "get_processor()",
            "self.processor"
        )
        
        # S'assurer que __init__ valide le processeur
        init_pattern = r'def __init__\(self, processor, config: PatchProcessorConfig\):'
        if init_pattern in content and 'if processor is None:' not in content:
            content = re.sub(
                r'(def __init__\(self, processor, config: PatchProcessorConfig\):\s*)(.*?\n\s*self\.processor = processor)',
                r'\1"""Initialise le wizard avec validation"""\n        if processor is None:\n            raise ValueError("Processor cannot be None")\n        \n        \2',
                content,
                flags=re.DOTALL
            )
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        self.fixes_applied.append("wizard_mode.py: Imports circulaires corrig√©s")
    
    def _fix_corrector_duplicates(self):
        """Corrige les m√©thodes dupliqu√©es dans line_number_corrector.py"""
        file_path = self.script_dir / "line_number_corrector.py"
        if not file_path.exists():
            return
            
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Compter les occurrences de correct_diff_headers
        method_count = content.count("def correct_diff_headers(")
        
        if method_count > 1:
            # Garder seulement la premi√®re d√©finition compl√®te avec validation
            lines = content.split('\n')
            result_lines = []
            in_first_method = False
            method_count_seen = 0
            indent_level = 0
            
            for line in lines:
                if "def correct_diff_headers(" in line:
                    method_count_seen += 1
                    if method_count_seen == 1:
                        in_first_method = True
                        indent_level = len(line) - len(line.lstrip())
                        result_lines.append(line)
                    else:
                        # Ignorer les d√©finitions suivantes
                        continue
                elif in_first_method:
                    # Continuer jusqu'√† la fin de la m√©thode
                    if line.strip() and not line.startswith(' ' * (indent_level + 1)) and not line.startswith(' ' * indent_level):
                        in_first_method = False
                        result_lines.append(line)
                    else:
                        result_lines.append(line)
                elif method_count_seen == 1 and not in_first_method:
                    # Apr√®s la premi√®re m√©thode, garder tout sauf les autres d√©finitions
                    if "def correct_diff_headers(" not in line:
                        result_lines.append(line)
                else:
                    result_lines.append(line)
            
            content = '\n'.join(result_lines)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.fixes_applied.append("line_number_corrector.py: M√©thodes dupliqu√©es supprim√©es")
    
    def _fix_applicator_logic(self):
        """Corrige la logique dans patch_applicator.py"""
        file_path = self.script_dir / "patch_applicator.py"
        if not file_path.exists():
            return
            
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Corriger les √©chappements de regex incorrects
        content = content.replace(r"r'@@\\s*-(\\d+)", r"r'@@\s*-(\d+)")
        content = content.replace(r"split('\\n')", r"split('\n')")
        content = content.replace(r"'\\n'", r"'\n'")
        
        # Corriger la logique conditionnelle YAML
        bad_yaml_logic = r'yaml\.dump\(" if yaml else "yaml\.dump\("'
        content = re.sub(bad_yaml_logic, 'yaml.dump(" if yaml else "# yaml not available"', content)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        self.fixes_applied.append("patch_applicator.py: Regex et logique corrig√©es")
    
    def _consolidate_files(self):
        """Consolide les fichiers selon les groupes logiques"""
        print("\nüì¶ Phase 3: Consolidation des fichiers")
        
        # 1. Consolider le syst√®me de cache
        self._consolidate_cache_system()
        
        # 2. Consolider le syst√®me de streaming
        self._consolidate_streaming_system()
        
        # 3. Consolider les types de base
        self._consolidate_core_types()
        
        print(f"   ‚úÖ {len(self.files_created)} fichiers consolid√©s cr√©√©s")
    
    def _consolidate_cache_system(self):
        """Consolide tous les fichiers du syst√®me de cache"""
        cache_files = [
            "dummy_cache.py", "cache_entry.py", "cache_stats.py", 
            "cache_strategy.py", "smart_cache.py"
        ]
        
        consolidated_content = '''"""
Syst√®me de cache consolid√© pour Smart Patch Processor
Regroupe: DummyCache, CacheEntry, CacheStats, CacheStrategy, SmartCache
"""

import logging
import threading
from pathlib import Path
from datetime import datetime
from collections import OrderedDict
from enum import Enum
from dataclasses import dataclass, field
import hashlib

# ============================================================================
# TYPES ET ENUMS
# ============================================================================

class CacheStrategy(Enum):
    """Strat√©gies de cache disponibles"""
    LRU = "lru"
    LFU = "lfu" 
    TTL = "ttl"
    HYBRID = "hybrid"
    ADAPTIVE = "adaptive"

@dataclass
class CacheStats:
    """Statistiques de performance du cache"""
    hits: int = 0
    misses: int = 0
    evictions: int = 0
    total_entries: int = 0
    hit_rate: float = 0.0
    
    def update_hit_rate(self):
        total = self.hits + self.misses
        self.hit_rate = (self.hits / total * 100) if total > 0 else 0.0

@dataclass
class CacheEntry:
    """Entr√©e de cache avec m√©tadonn√©es"""
    key: str
    value: Any
    created_at: datetime = field(default_factory=datetime.now)
    last_accessed: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    ttl_seconds: Optional[int] = None
    file_mtime: Optional[float] = None
    size_bytes: Optional[int] = None
    tags: List[str] = field(default_factory=list)
    
    @property
    def is_expired(self) -> bool:
        if self.ttl_seconds is None:
            return False
        age = (datetime.now() - self.created_at).total_seconds()
        return age > self.ttl_seconds
    
    def touch(self):
        self.last_accessed = datetime.now()
        self.access_count += 1

# ============================================================================
# IMPLEMENTATIONS DE CACHE
# ============================================================================

class DummyCache:
    """Cache factice qui ne cache rien (fallback)"""
    def put(self, key: str, value: Any, **kwargs) -> bool: 
        return False
    def get(self, key: str) -> None: 
        return None
    def clear(self): 
        pass

class SmartCache:
    """Cache intelligent int√©gr√© pour Smart Patch Processor"""
    
    def __init__(self, max_size: int = 1000, default_ttl: int = 3600):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self._lock = threading.RLock()
        self.stats = CacheStats()
        self.logger = logging.getLogger('smart_patch_processor.cache')
    
    def put(self, key: str, value: Any, ttl: Optional[int] = None, file_path: Optional[Path] = None) -> bool:
        with self._lock:
            effective_ttl = ttl or self.default_ttl
            file_mtime = file_path.stat().st_mtime if file_path and file_path.exists() else None
            
            entry = CacheEntry(
                key=key, value=value, created_at=datetime.now(),
                last_accessed=datetime.now(), access_count=1,
                ttl_seconds=effective_ttl, file_mtime=file_mtime
            )
            
            self._cache[key] = entry
            self._enforce_size_limits()
            return True
    
    def get(self, key: str) -> Optional[Any]:
        with self._lock:
            entry = self._cache.get(key)
            if entry is None:
                self.stats.misses += 1
                return None
            
            if entry.is_expired:
                self._cache.pop(key)
                self.stats.misses += 1
                return None
            
            entry.touch()
            self._cache.move_to_end(key)
            self.stats.hits += 1
            self.stats.update_hit_rate()
            return entry.value
    
    def _enforce_size_limits(self):
        while len(self._cache) > self.max_size:
            self._cache.popitem(last=False)
            self.stats.evictions += 1
    
    def clear(self):
        with self._lock:
            self._cache.clear()
            self.stats = CacheStats()

# ============================================================================
# GESTIONNAIRE GLOBAL
# ============================================================================

class CacheManager:
    """Gestionnaire global des caches"""
    
    def __init__(self, config):
        self.config = config
        cache_config = config.get_section('cache') if hasattr(config, 'get_section') else {}
        self.enabled = cache_config.get('enabled', True)
        
        if self.enabled:
            self.ast_cache = SmartCache(max_size=500, default_ttl=7200)
            self.detection_cache = SmartCache(max_size=200, default_ttl=3600)
            self.correction_cache = SmartCache(max_size=300, default_ttl=1800)
        else:
            self.ast_cache = self.detection_cache = self.correction_cache = DummyCache()
    
    def get_cache_key(self, prefix: str, *args) -> str:
        key_parts = [prefix] + [str(arg) for arg in args]
        key = ":".join(key_parts)
        if len(key) > 200:
            return f"{prefix}:hash:{hashlib.md5(key.encode()).hexdigest()}"
        return key
'''
        
        cache_system_file = self.script_dir / "cache_system.py"
        with open(cache_system_file, 'w', encoding='utf-8') as f:
            f.write(consolidated_content)
        
        self.files_created.append("cache_system.py")
    
    def _consolidate_streaming_system(self):
        """Consolide le syst√®me de streaming"""
        streaming_content = '''"""
Syst√®me de streaming consolid√© pour Smart Patch Processor
Regroupe: StreamingConfig, StreamingStats, CircularBuffer, StreamingFileReader, StreamingManager
"""

import logging
import threading
import mmap
import time
import weakref
from pathlib import Path
from contextlib import contextmanager
from dataclasses import dataclass

# ============================================================================
# CONFIGURATION ET STATS
# ============================================================================

@dataclass
class StreamingConfig:
    """Configuration pour le streaming de fichiers"""
    large_file_threshold_mb: int = 50
    huge_file_threshold_mb: int = 500
    default_chunk_size: int = 8192
    large_chunk_size: int = 65536
    huge_chunk_size: int = 1048576
    max_memory_usage_mb: int = 512
    buffer_pool_size: int = 10
    use_mmap: bool = True
    default_encoding: str = 'utf-8'
    encoding_errors: str = 'replace'

@dataclass
class StreamingStats:
    """Statistiques de performance du streaming"""
    files_processed: int = 0
    bytes_processed: int = 0
    streaming_files: int = 0
    mmap_files: int = 0
    memory_saved_mb: float = 0.0
    avg_processing_time_ms: float = 0.0

# ============================================================================
# BUFFER CIRCULAIRE
# ============================================================================

class CircularBuffer:
    """Buffer circulaire optimis√© pour le streaming"""
    
    def __init__(self, size: int):
        self.size = size
        self.buffer = bytearray(size)
        self.start = 0
        self.end = 0
        self.is_full = False
        self._lock = threading.Lock()
    
    def write(self, data: bytes) -> int:
        with self._lock:
            if not data or self.is_full:
                return 0
            
            data_len = min(len(data), self.size - self.available_data())
            if data_len == 0:
                return 0
            
            if self.end + data_len <= self.size:
                self.buffer[self.end:self.end + data_len] = data[:data_len]
                self.end = (self.end + data_len) % self.size
            else:
                first_part = self.size - self.end
                self.buffer[self.end:] = data[:first_part]
                self.buffer[:data_len - first_part] = data[first_part:data_len]
                self.end = data_len - first_part
            
            if self.end == self.start:
                self.is_full = True
            return data_len
    
    def read(self, size: int) -> bytes:
        with self._lock:
            if self.start == self.end and not self.is_full:
                return b''
            
            available = self.size if self.is_full else (self.end - self.start) % self.size
            to_read = min(size, available)
            
            if self.start + to_read <= self.size:
                result = bytes(self.buffer[self.start:self.start + to_read])
                self.start = (self.start + to_read) % self.size
            else:
                first_part = self.size - self.start
                result = bytes(self.buffer[self.start:] + self.buffer[:to_read - first_part])
                self.start = to_read - first_part
            
            self.is_full = False
            return result
    
    def available_data(self) -> int:
        if self.is_full:
            return self.size
        return (self.end - self.start) % self.size
    
    def clear(self):
        with self._lock:
            self.start = 0
            self.end = 0
            self.is_full = False

# ============================================================================
# LECTEUR DE FICHIER STREAMING
# ============================================================================

class StreamingFileReader:
    """Lecteur de fichier avec streaming intelligent"""
    
    def __init__(self, file_path: Path, config: StreamingConfig):
        self.file_path = file_path
        self.config = config
        self._file_handle = None
        self._mmap = None
        self._size = file_path.stat().st_size if file_path.exists() else 0
        self._use_mmap = (self._size > config.huge_file_threshold_mb * 1024 * 1024 
                         and config.use_mmap)
        self._buffer = CircularBuffer(config.large_chunk_size)
    
    def read_chunks(self, chunk_size: int = None):
        """G√©n√®re les chunks du fichier"""
        chunk_size = chunk_size or self.config.default_chunk_size
        
        if self._use_mmap:
            yield from self._read_chunks_mmap(chunk_size)
        else:
            yield from self._read_chunks_standard(chunk_size)
    
    def _read_chunks_mmap(self, chunk_size: int):
        """Lecture via mmap pour tr√®s gros fichiers"""
        try:
            with open(self.file_path, 'rb') as f:
                self._file_handle = f
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                    self._mmap = mm
                    position = 0
                    
                    while position < len(mm):
                        chunk = mm[position:position + chunk_size]
                        position += len(chunk)
                        yield chunk
        finally:
            self._mmap = None
            self._file_handle = None
    
    def _read_chunks_standard(self, chunk_size: int):
        """Lecture standard avec buffer"""
        try:
            with open(self.file_path, 'rb') as f:
                self._file_handle = f
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    yield chunk
        finally:
            self._file_handle = None
    
    def get_file_size(self) -> int:
        return self._size
    
    def close(self):
        if self._mmap:
            self._mmap.close()
        if self._file_handle:
            self._file_handle.close()

# ============================================================================
# GESTIONNAIRE DE STREAMING
# ============================================================================

class StreamingManager:
    """Gestionnaire du streaming de fichiers"""
    
    def __init__(self, config: Optional[StreamingConfig] = None):
        self.config = config or StreamingConfig()
        self.stats = StreamingStats()
        self.logger = logging.getLogger('smart_patch_processor.streaming')
        self._active_readers = weakref.WeakSet()
    
    def should_use_streaming(self, file_path: Path) -> bool:
        """D√©termine si un fichier doit utiliser le streaming"""
        if not file_path.exists():
            return False
        
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        return file_size_mb > self.config.large_file_threshold_mb
    
    def create_reader(self, file_path: Path) -> StreamingFileReader:
        """Cr√©e un lecteur streaming pour un fichier"""
        reader = StreamingFileReader(file_path, self.config)
        self._active_readers.add(reader)
        
        if self.should_use_streaming(file_path):
            self.stats.streaming_files += 1
            if reader._use_mmap:
                self.stats.mmap_files += 1
        
        return reader
    
    @contextmanager
    def streaming_context(self, file_path: Path):
        """Context manager pour g√©rer automatiquement un lecteur"""
        reader = None
        start_time = time.time()
        
        try:
            reader = self.create_reader(file_path)
            yield reader
        finally:
            if reader:
                processing_time = time.time() - start_time
                self.stats.files_processed += 1
                self.stats.bytes_processed += reader.get_file_size()
                self.stats.avg_processing_time_ms = (
                    (self.stats.avg_processing_time_ms * 0.9) +
                    (processing_time * 1000 * 0.1)
                )
                reader.close()
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du streaming"""
        return {
            'files_processed': self.stats.files_processed,
            'bytes_processed': self.stats.bytes_processed,
            'streaming_files': self.stats.streaming_files,
            'mmap_files': self.stats.mmap_files,
            'memory_saved_mb': self.stats.memory_saved_mb,
            'avg_processing_time_ms': self.stats.avg_processing_time_ms
        }
'''
        
        streaming_system_file = self.script_dir / "streaming_system.py"
        with open(streaming_system_file, 'w', encoding='utf-8') as f:
            f.write(streaming_content)
        
        self.files_created.append("streaming_system.py")
    
    def _consolidate_core_types(self):
        """Consolide les types de base"""
        types_content = '''"""
Types de base consolid√©s pour Smart Patch Processor
Regroupe: IssueType, LanguageType, LanguageInfo, PatchIssue
"""

from enum import Enum
from dataclasses import dataclass, field

# ============================================================================
# ENUMS DE BASE
# ============================================================================

class IssueType(Enum):
    """Types d'issues d√©tect√©es"""
    ERROR = "error"
    WARNING = "warning"
    INFO = "info"

class LanguageType(Enum):
    """Types de langages support√©s"""
    PYTHON = "python"
    JAVASCRIPT = "javascript"
    TYPESCRIPT = "typescript" 
    PHP = "php"
    JAVA = "java"
    CPP = "cpp"
    C = "c"
    GO = "go"
    RUST = "rust"
    KOTLIN = "kotlin"
    SWIFT = "swift"
    RUBY = "ruby"
    CSHARP = "csharp"
    UNKNOWN = "unknown"

# ============================================================================
# CLASSES DE DONN√âES
# ============================================================================

@dataclass
class LanguageInfo:
    """Informations sur un langage"""
    name: str
    type: LanguageType
    extensions: List[str]
    analyzer_class: str
    features: List[str]
    complexity: int

@dataclass
class PatchIssue:
    """Repr√©sente un probl√®me d√©tect√© dans un patch"""
    type: IssueType
    line_number: int
    message: str
    suggestion: Optional[str] = None
    auto_fixable: bool = False
    severity: int = 1  # 1=low, 2=medium, 3=high

    def to_dict(self) -> Dict:
        """Convertir en dictionnaire pour s√©rialisation"""
        return {
            'type': self.type.value,
            'line_number': self.line_number,
            'message': self.message,
            'suggestion': self.suggestion,
            'auto_fixable': self.auto_fixable,
            'severity': self.severity
        }

# ============================================================================
# UTILITAIRES
# ============================================================================

def get_language_from_extension(extension: str) -> LanguageType:
    """D√©tecte le langage depuis l'extension de fichier"""
    mapping = {
        '.py': LanguageType.PYTHON,
        '.js': LanguageType.JAVASCRIPT,
        '.jsx': LanguageType.JAVASCRIPT,
        '.ts': LanguageType.TYPESCRIPT,
        '.tsx': LanguageType.TYPESCRIPT,
        '.php': LanguageType.PHP,
        '.java': LanguageType.JAVA,
        '.cpp': LanguageType.CPP,
        '.cxx': LanguageType.CPP,
        '.cc': LanguageType.CPP,
        '.c': LanguageType.C,
        '.h': LanguageType.C,
        '.go': LanguageType.GO,
        '.rs': LanguageType.RUST,
        '.kt': LanguageType.KOTLIN,
        '.swift': LanguageType.SWIFT,
        '.rb': LanguageType.RUBY,
        '.cs': LanguageType.CSHARP,
    }
    return mapping.get(extension.lower(), LanguageType.UNKNOWN)

def get_supported_extensions() -> Dict[LanguageType, List[str]]:
    """Retourne les extensions support√©es par langage"""
    return {
        LanguageType.PYTHON: ['.py', '.pyi', '.pyw'],
        LanguageType.JAVASCRIPT: ['.js', '.jsx', '.mjs'],
        LanguageType.TYPESCRIPT: ['.ts', '.tsx', '.d.ts'],
        LanguageType.PHP: ['.php', '.php5', '.phtml'],
        LanguageType.JAVA: ['.java'],
        LanguageType.CPP: ['.cpp', '.cxx', '.cc', '.hpp', '.hxx'],
        LanguageType.C: ['.c', '.h'],
        LanguageType.GO: ['.go'],
        LanguageType.RUST: ['.rs'],
        LanguageType.KOTLIN: ['.kt', '.kts'],
        LanguageType.SWIFT: ['.swift'],
        LanguageType.RUBY: ['.rb', '.rbw'],
        LanguageType.CSHARP: ['.cs'],
    }
'''
        
        core_types_file = self.script_dir / "core_types.py"
        with open(core_types_file, 'w', encoding='utf-8') as f:
            f.write(types_content)
        
        self.files_created.append("core_types.py")
    
    def _remove_redundant_files(self):
        """Supprime les fichiers redondants et temporaires"""
        print("\nüóëÔ∏è Phase 4: Suppression des fichiers redondants")
        
        # Fichiers √† supprimer apr√®s consolidation
        files_to_remove = [
            # Syst√®me de cache (consolid√© dans cache_system.py)
            "dummy_cache.py", "cache_entry.py", "cache_stats.py", 
            "cache_strategy.py", "smart_cache.py", "cache_manager.py",
            
            # Syst√®me de streaming (consolid√© dans streaming_system.py)
            "streaming_config.py", "streaming_stats.py", "circular_buffer.py",
            "streaming_file_reader.py", "streaming_manager.py",
            
            # Types de base (consolid√© dans core_types.py)
            "issue_type.py", "language_type.py", "language_info.py", "patch_issue.py",
            
            # Fichiers redondants/inutiles
            "run_smart_patch.py",  # Redondant avec main.py et smart-patch
            
            # Scripts temporaires/correction (apr√®s application)
            "smart_patch_fixes.py", "architecture_fixer.py",
            
            # Potentiellement le g√©n√©rateur de config (peut √™tre int√©gr√©)
            "advanced_config_generator.py",
        ]
        
        for file_name in files_to_remove:
            file_path = self.script_dir / file_name
            if file_path.exists():
                file_path.unlink()
                self.files_deleted.append(file_name)
        
        print(f"   ‚úÖ {len(self.files_deleted)} fichiers redondants supprim√©s")
    
    def _apply_final_optimizations(self):
        """Applique les optimisations finales"""
        print("\n‚ö° Phase 5: Optimisations finales")
        
        # Mettre √† jour les imports dans les fichiers qui utilisent les modules consolid√©s
        self._update_imports_for_consolidation()
        
        # Nettoyer les imports inutilis√©s
        self._clean_unused_imports()
        
        # Optimiser les imports longs
        self._optimize_long_imports()
        
        print(f"   ‚úÖ Optimisations appliqu√©es")
    
    def _update_imports_for_consolidation(self):
        """Met √† jour les imports pour pointer vers les fichiers consolid√©s"""
        
        # Mappings des anciens imports vers les nouveaux
        import_mappings = {
            # Cache system
            'from cache_system import DummyCache': 'from cache_system import DummyCache',
            'from cache_system import CacheEntry': 'from cache_system import CacheEntry',
            'from cache_system import CacheStats': 'from cache_system import CacheStats',
            'from cache_system import CacheStrategy': 'from cache_system import CacheStrategy',
            'from cache_system import SmartCache': 'from cache_system import SmartCache',
            'from cache_system import CacheManager': 'from cache_system import CacheManager',
            
            # Streaming system
            'from streaming_system import StreamingConfig': 'from streaming_system import StreamingConfig',
            'from streaming_system import StreamingStats': 'from streaming_system import StreamingStats',
            'from streaming_system import CircularBuffer': 'from streaming_system import CircularBuffer',
            'from streaming_system import StreamingFileReader': 'from streaming_system import StreamingFileReader',
            'from streaming_system import StreamingManager': 'from streaming_system import StreamingManager',
            
            # Core types
            'from core_types import IssueType': 'from core_types import IssueType',
            'from core_types import LanguageType': 'from core_types import LanguageType',
            'from core_types import LanguageInfo': 'from core_types import LanguageInfo',
            'from core_types import PatchIssue': 'from core_types import PatchIssue',
        }
        
        # Appliquer les mappings √† tous les fichiers Python restants
        for py_file in self.script_dir.glob("*.py"):
            if py_file.name not in [f.name for f in [self.backup_dir] if f.is_dir()]:
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    original_content = content
                    for old_import, new_import in import_mappings.items():
                        content = content.replace(old_import, new_import)
                    
                    if content != original_content:
                        with open(py_file, 'w', encoding='utf-8') as f:
                            f.write(content)
                        self.fixes_applied.append(f"{py_file.name}: Imports mis √† jour")
                        
                except Exception as e:
                    self.logger.warning(f"Erreur mise √† jour imports {py_file.name}: {e}")
    
    def _clean_unused_imports(self):
        """Nettoie les imports non utilis√©s basiques"""
        for py_file in self.script_dir.glob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # D√©tecter et supprimer quelques imports manifestement inutilis√©s
                lines = content.split('\n')
                cleaned_lines = []
                
                for line in lines:
                    # Supprimer les imports de modules supprim√©s
                    if any(deleted in line for deleted in self.files_deleted):
                        if line.strip().startswith(('import ', 'from ')):
                            continue  # Ignorer cette ligne d'import
                    cleaned_lines.append(line)
                
                cleaned_content = '\n'.join(cleaned_lines)
                
                if cleaned_content != content:
                    with open(py_file, 'w', encoding='utf-8') as f:
                        f.write(cleaned_content)
                    self.fixes_applied.append(f"{py_file.name}: Imports nettoy√©s")
                    
            except Exception as e:
                self.logger.warning(f"Erreur nettoyage imports {py_file.name}: {e}")
    
    def _optimize_long_imports(self):
        """Optimise les imports longs en les regroupant"""
        for py_file in self.script_dir.glob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Regrouper les imports de typing
                typing_imports = []
                other_lines = []
                
                for line in content.split('\n'):
                    if line.strip().startswith('from typing import '):
                        imports = line.replace('from typing import ', '').strip()
                        typing_imports.extend([imp.strip() for imp in imports.split(',')])
                    else:
                        other_lines.append(line)
                
                if len(typing_imports) > 1:
                    # Reconstruire avec un seul import typing
                    unique_imports = sorted(set(typing_imports))
                    new_typing_line = f"from typing import {', '.join(unique_imports)}"
                    
                    # Ins√©rer au bon endroit (apr√®s les autres imports)
                    new_lines = []
                    typing_inserted = False
                    
                    for line in other_lines:
                        if not typing_inserted and line.strip() and not line.startswith(('import ', 'from ')):
                            new_lines.append(new_typing_line)
                            typing_inserted = True
                        new_lines.append(line)
                    
                    if not typing_inserted:
                        new_lines.insert(0, new_typing_line)
                    
                    new_content = '\n'.join(new_lines)
                    
                    if new_content != content:
                        with open(py_file, 'w', encoding='utf-8') as f:
                            f.write(new_content)
                        self.fixes_applied.append(f"{py_file.name}: Imports typing optimis√©s")
                        
            except Exception as e:
                self.logger.warning(f"Erreur optimisation imports {py_file.name}: {e}")
    
    def _validate_fixes(self):
        """Valide que les corrections ont √©t√© appliqu√©es correctement"""
        print("\nüß™ Phase 6: Validation des corrections")
        
        validation_results = []
        
        # Test 1: V√©rifier les imports Python de base
        try:
            test_imports = [
                "import sys",
                "from pathlib import Path",
                "import json",
            ]
            
            for test_import in test_imports:
                exec(test_import)
            validation_results.append("‚úÖ Imports Python de base: OK")
            
        except Exception as e:
            validation_results.append(f"‚ùå Imports Python de base: {e}")
        
        # Test 2: V√©rifier que les fichiers consolid√©s sont valides
        consolidated_files = ["cache_system.py", "streaming_system.py", "core_types.py"]
        
        for file_name in consolidated_files:
            file_path = self.script_dir / file_name
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # V√©rification syntaxique de base
                    compile(content, file_name, 'exec')
                    validation_results.append(f"‚úÖ {file_name}: Syntaxe valide")
                    
                except SyntaxError as e:
                    validation_results.append(f"‚ùå {file_name}: Erreur syntaxe ligne {e.lineno}")
                except Exception as e:
                    validation_results.append(f"‚ùå {file_name}: {e}")
            else:
                validation_results.append(f"‚ùå {file_name}: Fichier manquant")
        
        # Test 3: V√©rifier les fichiers principaux
        main_files = ["main.py", "smart_patch_processor.py", "wizard_mode.py"]
        
        for file_name in main_files:
            file_path = self.script_dir / file_name
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # V√©rifications sp√©cifiques
                    if file_name == "validation.py" and "original_filename" in content:
                        validation_results.append("‚úÖ validation.py: Variable original_filename pr√©sente")
                    
                    if file_name == "wizard_mode.py" and "get_processor()" not in content:
                        validation_results.append("‚úÖ wizard_mode.py: Import circulaire r√©solu")
                    
                    # V√©rification syntaxique
                    compile(content, file_name, 'exec')
                    validation_results.append(f"‚úÖ {file_name}: OK")
                    
                except SyntaxError as e:
                    validation_results.append(f"‚ùå {file_name}: Erreur syntaxe ligne {e.lineno}")
                except Exception as e:
                    validation_results.append(f"‚ùå {file_name}: {e}")
        
        # Afficher les r√©sultats
        for result in validation_results:
            print(f"   {result}")
        
        # Compter les succ√®s
        success_count = len([r for r in validation_results if r.startswith("‚úÖ")])
        total_count = len(validation_results)
        
        print(f"\n   üìä Validation: {success_count}/{total_count} tests r√©ussis")
        
        if success_count < total_count * 0.8:  # Moins de 80% de succ√®s
            print(f"   ‚ö†Ô∏è Validation partielle - v√©rifiez les erreurs ci-dessus")
        else:
            print(f"   ‚úÖ Validation r√©ussie !")
    
    def _show_summary(self):
        """Affiche le r√©sum√© complet des corrections"""
        print("\n" + "=" * 70)
        print("üìä R√âSUM√â COMPLET DES CORRECTIONS")
        print("=" * 70)
        
        print(f"\nüö® ERREURS CORRIG√âES ({len(self.fixes_applied)}):")
        for fix in self.fixes_applied:
            print(f"   ‚úÖ {fix}")
        
        print(f"\nüì¶ FICHIERS CONSOLID√âS ({len(self.files_created)}):")
        for file in self.files_created:
            print(f"   üìÑ {file}")
        
        print(f"\nüóëÔ∏è FICHIERS SUPPRIM√âS ({len(self.files_deleted)}):")
        for file in self.files_deleted:
            print(f"   üóëÔ∏è {file}")
        
        print(f"\nüìà STATISTIQUES:")
        before_files = len(list(self.backup_dir.glob("*.py")))
        after_files = len(list(self.script_dir.glob("*.py")))
        reduction = before_files - after_files
        reduction_pct = (reduction / before_files * 100) if before_files > 0 else 0
        
        print(f"   üìÅ Fichiers avant: {before_files}")
        print(f"   üìÅ Fichiers apr√®s: {after_files}")
        print(f"   üìâ R√©duction: {reduction} fichiers (-{reduction_pct:.1f}%)")
        
        print(f"\nüíæ SAUVEGARDE:")
        print(f"   üìÅ Backup complet dans: {self.backup_dir}")
        print(f"   üîÑ Restauration possible avec: cp {self.backup_dir}/* ./")
        
        print(f"\nüß™ TESTS RECOMMAND√âS:")
        tests = [
            "python3 -c \"import cache_system; print('Cache system OK')\"",
            "python3 -c \"import streaming_system; print('Streaming system OK')\"", 
            "python3 -c \"import core_types; print('Core types OK')\"",
            "python3 -c \"from smart_patch_processor import SmartPatchProcessor; print('Main import OK')\"",
            "python3 main.py --help",
            "python3 main.py --wizard"
        ]
        
        for test in tests:
            print(f"   üß™ {test}")
        
        print(f"\nüéâ CORRECTION COMPL√àTE TERMIN√âE !")
        print(f"üí° Votre codebase est maintenant plus propre, optimis√©e et sans erreurs")
    
    def _restore_backup(self):
        """Restaure les fichiers depuis la sauvegarde en cas d'erreur"""
        print(f"\nüîÑ Restauration depuis la sauvegarde...")
        
        try:
            # Supprimer les fichiers actuels
            for py_file in self.script_dir.glob("*.py"):
                if py_file.name != Path(__file__).name:
                    py_file.unlink()
            
            # Restaurer depuis la sauvegarde
            for backup_file in self.backup_dir.glob("*.py"):
                shutil.copy2(backup_file, self.script_dir)
            
            print(f"   ‚úÖ Fichiers restaur√©s depuis {self.backup_dir}")
            
        except Exception as e:
            print(f"   ‚ùå Erreur lors de la restauration: {e}")


def main():
    """Point d'entr√©e principal"""
    if len(sys.argv) > 1 and sys.argv[1] in ['-h', '--help']:
        print("""
üîß Correcteur Complet Smart Patch Processor v3.0

USAGE:
    python3 comprehensive_fixer.py

DESCRIPTION:
    Corrige automatiquement TOUTES les erreurs identifi√©es dans le Smart Patch
    Processor et optimise l'architecture en consolidant les fichiers.

CORRECTIONS APPLIQU√âES:
    üö® Erreurs critiques (variables non d√©finies, imports circulaires, etc.)
    üì¶ Consolidation de fichiers (cache, streaming, types)
    üóëÔ∏è Suppression des fichiers redondants
    ‚ö° Optimisations des imports
    üß™ Validation compl√®te

AVANT/APR√àS:
    üìÅ ~61 fichiers ‚Üí ~45 fichiers (-26%)
    üêõ ~15 erreurs ‚Üí 0 erreur
    üìä Code plus maintenable et optimis√©

S√âCURIT√â:
    ‚úÖ Backup automatique complet
    ‚úÖ Restauration possible en cas d'erreur
    ‚úÖ Validation des corrections
        """)
        return
    
    print("‚ö†Ô∏è  ATTENTION: Ce script va modifier de nombreux fichiers !")
    print("‚úÖ Un backup complet sera cr√©√© automatiquement")
    print()
    
    response = input("Continuer avec la correction compl√®te ? (y/N): ").strip().lower()
    if response != 'y':
        print("‚ùå Correction annul√©e")
        return
    
    fixer = ComprehensiveFixer()
    success = fixer.run_comprehensive_fix()
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()